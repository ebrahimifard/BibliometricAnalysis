{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import operator\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addressBook = {\"serializedData\" : \"D:/PhD/Projects/Defected Info Bibliometrics Study/Special Issue Effort/Analysis/ProcessedData/SerializedData/\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPapers = pk.loads(open(addressBook[\"serializedData\"]+\"allPapers.dic\", \"rb\").read())\n",
    "annualPapers = pk.loads(open(addressBook[\"serializedData\"]+\"annualPapers.dic\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING THE STOPWORDS LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords1 = set(stopwords.words('english'))\n",
    "stopWords2 = stop_words.get_stop_words('en')\n",
    "for a in string.ascii_letters:\n",
    "    stopWords2.append(a)\n",
    "for i in range(0,400):\n",
    "    stopWords2.append(str(i))\n",
    "for i in [\"One\",\"Two\",\"Three\",\"Four\",\"Five\",\"Six\",\"Seven\",\"Eight\",\"Nine\",\"Ten\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\",\"also\",\"since\",\"paper\",\"article\",\"result\", \"study\"]:\n",
    "    stopWords2.append(i)\n",
    "stopwords = list(set(list(stopWords1) + stopWords2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHRASE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "sentence_stream = []\n",
    "for pid in tqdm(allPapers):\n",
    "    ptitle = allPapers[pid][\"TI\"]\n",
    "    pabstract = allPapers[pid][\"AB\"]\n",
    "    pauthorKeywords = allPapers[pid][\"DE\"].replace(\";\", \" \")\n",
    "    pplusKeywords = allPapers[pid][\"ID\"].replace(\";\", \" \")\n",
    "    pcontent = pauthorKeywords + pplusKeywords + ptitle + pabstract\n",
    "    documents.append(pcontent)\n",
    "sentence_stream.extend([doc.split(\" \") for doc in documents])\n",
    "bigram = Phrases(sentence_stream, min_count=1, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicNum in range(1,6):\n",
    "    LDAs = {}\n",
    "    wordNum = 5\n",
    "    passNum = 50\n",
    "    texts = {\"1900~1980\":[],\"1980~1990\":[],\"1990~2000\":[],\"2000~2010\":[],\"2010~2018\":[]}\n",
    "    adjustedTexts = {\"1900~1980\": [], \"1980~1990\": [], \"1990~2000\": [], \"2000~2010\": [], \"2010~2018\": []}\n",
    "    wholetexts = []\n",
    "\n",
    "    for paper_id in allPapers:\n",
    "            title = allPapers[paper_id][\"TI\"]\n",
    "            abstract = allPapers[paper_id][\"AB\"]\n",
    "            authorKeywords = allPapers[paper_id][\"DE\"].replace(\";\", \" \")\n",
    "            plusKeywords = allPapers[paper_id][\"ID\"].replace(\";\", \" \")\n",
    "            content = (authorKeywords + plusKeywords + title + abstract).lower()\n",
    "            content = content.replace(\"rumor\",\"\").replace(\"rumour\",\"\").replace(\"fallacy\",\"\").replace(\"fallacies\",\"\").replace(\"misinformation\",\"\").replace(\"gossip\",\"\").replace(\"pseudoscience\",\"\").replace(\"smear campaigns\",\"\").replace(\"smear campaign\",\"\").replace(\"factchecking\",\"\").replace(\"fact checking\",\"\"). replace(\"fact-checking\",\"\").replace(\"fact-check\",\"\").replace(\"fact check\",\"\").replace(\"factcheck\",\"\").replace(\"truthiness\",\"\").replace(\"media manipulation\",\"\").replace(\"urban-legend\",\"\").replace(\"urban legend\",\"\").replace(\"alternative facts\",\"\").replace(\"alternative fact\",\"\").replace(\"post truth\", \"\").replace(\"kompromat\",\"\").replace(\"false flag\",\"\").replace(\"denial and deception\", \"\").replace(\"propaganda\",\"\").replace(\"fallacy\",\"\").replace(\"factoid\",\"\").replace(\"disinformation\",\"\").replace(\"defamation\",\"\").replace(\"character assassination\",\"\").replace(\"common misconceptions\",\"\").replace(\"common misconception\",\"\").replace(\"false dilemma\",\"\").replace(\"fake news\",\"\").replace(\"fake-news\",\"\").replace(\"hoax\",\"\").replace(\"blind items\",\"\").replace(\"blind item\",\"\")       \n",
    "\n",
    "\n",
    "            #Preprocessing\n",
    "            tokens = list(gensim.utils.tokenize(content, deacc=True, lower=True))\n",
    "            lemmatizedTokens = [lemmatizer.lemmatize(lw) for lw in tokens if lw not in stopwords]\n",
    "            phrases = bigram[lemmatizedTokens]\n",
    "            lemmatizedTokens = list(set(lemmatizedTokens + phrases))\n",
    "\n",
    "            year = int(allPapers[paper_id][\"PY\"].strip())\n",
    "            if(year in [y for y in range(1900,1980)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"1900~1980\"].append(lemmatizedTokens)\n",
    "            elif(year in [y for y in range(1980,1990)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"1980~1990\"].append(lemmatizedTokens)\n",
    "            elif(year in [y for y in range(1990,2000)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"1990~2000\"].append(lemmatizedTokens)\n",
    "            elif(year in [y for y in range(2000,2010)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"2000~2010\"].append(lemmatizedTokens)\n",
    "            elif(year in [y for y in range(2010,2019)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"2010~2018\"].append(lemmatizedTokens)\n",
    "            if len(lemmatizedTokens)>0:\n",
    "                wholetexts.append(lemmatizedTokens)\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "    for text in wholetexts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    adjustedWholeText = [[token for token in text if frequency[token] > 1] for text in wholetexts]\n",
    "    freq = sorted_x = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    for key in texts:\n",
    "        adjustedTexts[key] = [[token for token in text if frequency[token] > 1] for text in texts[key]]\n",
    "\n",
    "    dictionary = corpora.Dictionary(adjustedWholeText)\n",
    "    corpus = [dictionary.doc2bow(text) for text in adjustedWholeText]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topicNum, id2word=dictionary, passes=passNum)\n",
    "    LDAs[\"total\"] = ldamodel\n",
    "    print(ldamodel.print_topics(num_topics=topicNum, num_words=wordNum))\n",
    "    LDA_temp = {\"total\": [], \"1900~1980\":[],\"1980~1990\":[],\"1990~2000\":[],\"2000~2010\":[],\"2010~2018\":[]}\n",
    "    df = pd.DataFrame(columns=[\"word\"+str(i) for i in range(1,topicNum*wordNum+1)])\n",
    "\n",
    "    for c,eachYear in enumerate(adjustedTexts):\n",
    "        if len(adjustedTexts[eachYear])>0:\n",
    "            dictionary = corpora.Dictionary(adjustedTexts[eachYear])\n",
    "            corpus = [dictionary.doc2bow(text) for text in adjustedTexts[eachYear]]\n",
    "            ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topicNum, id2word=dictionary, passes=passNum)\n",
    "            LDAs[eachYear] = ldamodel\n",
    "            print(str(eachYear) + \" : \")\n",
    "            print(ldamodel.print_topics(num_topics=topicNum, num_words=wordNum))\n",
    "\n",
    "    open(f'./LDA(firstVersion)_{str(passNum)}_{str(topicNum)}_{str(wordNum)}.dic', \"wb\").write(pk.dumps(LDAs))\n",
    "\n",
    "    for c,eachYear in enumerate(LDAs):\n",
    "        for i in range(0,topicNum):\n",
    "            LDA_temp[eachYear].extend([item.split(\"\\\"\")[1].strip().rstrip() for item in str(LDAs[eachYear].print_topics(num_topics=topicNum, num_words=wordNum)[i]).split(\"+\")])\n",
    "            while(len(LDA_temp[eachYear]) % wordNum != 0):\n",
    "                LDA_temp[eachYear].append(0)\n",
    "        df.loc[c] = [i for i in LDA_temp[eachYear]]\n",
    "    df.to_csv(f'./LDA_(firstVersion)_(Keywords+Title+Abstract){str(topicNum)}.csv', sep=';', decimal=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicNum in tqdm(range(1,6)):\n",
    "    LDAs = {}\n",
    "    wordNum = 5\n",
    "    passNum = 50\n",
    "    texts = {\"1900~1970\":[],\"1970~2018\":[]}\n",
    "    adjustedTexts = {\"1900~1970\": [], \"1970~2018\": []}\n",
    "    wholetexts = []\n",
    "\n",
    "    for paper_id in tqdm(allPapers):\n",
    "            title = allPapers[paper_id][\"TI\"]\n",
    "            abstract = allPapers[paper_id][\"AB\"]\n",
    "            authorKeywords = allPapers[paper_id][\"DE\"].replace(\";\", \" \")\n",
    "            plusKeywords = allPapers[paper_id][\"ID\"].replace(\";\", \" \")\n",
    "            content = (authorKeywords + plusKeywords + title + abstract).lower().replace(\"rumor\",\"rumour\").replace(\"rumour\",\"\").replace(\"fallacy\",\"\").replace(\"misinformation\",\"\").replace(\"gossip\",\"\").replace(\"pseudoscience\",\"\").replace(\"smear campaigns\",\"\").replace(\"smear campaign\",\"\").replace(\"factchecking\",\"\").replace(\"fact checking\",\"\"). replace(\"fact-checking\",\"\").replace(\"fact-check\",\"\").replace(\"fact check\",\"\").replace(\"factcheck\",\"\").replace(\"truthiness\",\"\").replace(\"media manipulation\",\"\").replace(\"urban-legend\",\"\").replace(\"urban legend\",\"\").replace(\"alternative facts\",\"\").replace(\"alternative fact\",\"\").replace(\"post truth\", \"\").replace(\"kompromat\",\"\").replace(\"false flag\",\"\").replace(\"denial and deception\", \"\").replace(\"propaganda\",\"\").replace(\"fallacy\",\"\").replace(\"factoid\",\"\").replace(\"disinformation\",\"\").replace(\"defamation\",\"\").replace(\"character assassination\",\"\").replace(\"common misconceptions\",\"\").replace(\"common misconception\",\"\").replace(\"false dilemma\",\"\").replace(\"fake news\",\"\").replace(\"fake-news\",\"\").replace(\"hoax\",\"\").replace(\"blind items\",\"\").replace(\"blind item\",\"\")       \n",
    "\n",
    "            #Preprocessing\n",
    "            tokens = list(gensim.utils.tokenize(content, deacc=True, lower=True))\n",
    "            lemmatizedTokens = [lemmatizer.lemmatize(lw) for lw in tokens if lw not in stopwords]\n",
    "            phrases = bigram[lemmatizedTokens]\n",
    "            lemmatizedTokens = list(set(lemmatizedTokens + phrases))\n",
    "\n",
    "            year = int(allPapers[paper_id][\"PY\"].strip())\n",
    "            if(year in [y for y in range(1900,1970)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"1900~1970\"].append(lemmatizedTokens)\n",
    "            elif(year in [y for y in range(1970,2019)]):\n",
    "                if len(lemmatizedTokens)>0:\n",
    "                    texts[\"1970~2018\"].append(lemmatizedTokens)\n",
    "            if len(lemmatizedTokens)>0:\n",
    "                wholetexts.append(lemmatizedTokens)\n",
    "\n",
    "    frequency = defaultdict(int)\n",
    "    for text in wholetexts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    adjustedWholeText = [[token for token in text if frequency[token] > 1] for text in wholetexts]\n",
    "    freq = sorted_x = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    for key in texts:\n",
    "        adjustedTexts[key] = [[token for token in text if frequency[token] > 1] for text in texts[key]]\n",
    "\n",
    "    dictionary = corpora.Dictionary(adjustedWholeText)\n",
    "    corpus = [dictionary.doc2bow(text) for text in adjustedWholeText]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topicNum, id2word=dictionary, passes=passNum)\n",
    "    LDAs[\"total\"] = ldamodel\n",
    "    print(ldamodel.print_topics(num_topics=topicNum, num_words=wordNum))\n",
    "    LDA_temp = {\"total\": [], \"1900~1970\":[],\"1970~2018\":[]}\n",
    "    df = pd.DataFrame(columns=[\"word\"+str(i) for i in range(1,topicNum*wordNum+1)])\n",
    "\n",
    "    for c,eachYear in enumerate(adjustedTexts):\n",
    "        if len(adjustedTexts[eachYear])>0:\n",
    "            dictionary = corpora.Dictionary(adjustedTexts[eachYear])\n",
    "            corpus = [dictionary.doc2bow(text) for text in adjustedTexts[eachYear]]\n",
    "            ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topicNum, id2word=dictionary, passes=passNum)\n",
    "            LDAs[eachYear] = ldamodel\n",
    "            print(str(eachYear) + \" : \")\n",
    "            print(ldamodel.print_topics(num_topics=topicNum, num_words=wordNum))\n",
    "\n",
    "    open(f'./LDA(SecondVersion)_{str(passNum)}_{str(topicNum)}_{str(wordNum)}.dic', \"wb\").write(pk.dumps(LDAs))\n",
    "\n",
    "    for c,eachYear in enumerate(LDAs):\n",
    "        for i in range(0,topicNum):\n",
    "            LDA_temp[eachYear].extend([item.split(\"\\\"\")[1].strip().rstrip() for item in str(LDAs[eachYear].print_topics(num_topics=topicNum, num_words=wordNum)[i]).split(\"+\")])\n",
    "            while(len(LDA_temp[eachYear]) % wordNum != 0):\n",
    "                LDA_temp[eachYear].append(0)\n",
    "        df.loc[c] = [i for i in LDA_temp[eachYear]]\n",
    "    df.to_csv(f'./LDA(FirstVersion)_(Keywords+Title+Abstract)_{str(topicNum)}.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the serialized LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(os.walk(\".\"))\n",
    "LDA = pk.loads(open(\"./LDA(firstVersion)_50_5_5.dic\",\"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1 = LDA[\"1900~1980\"]\n",
    "lda2 = LDA[\"1980~1990\"]\n",
    "lda3 = LDA[\"1990~2000\"]\n",
    "lda4= LDA[\"2000~2010\"]\n",
    "lda5= LDA[\"2010~2018\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "topicNum = 5\n",
    "id_counter = 1\n",
    "for year, lda in LDA.items():\n",
    "    for tn in tqdm(range(topicNum)):\n",
    "        elements = lda.show_topic(tn, topn=lda.num_terms)\n",
    "        for element in elements:\n",
    "            if element[0] not in word2id.keys():\n",
    "                word2id[element[0]] = id_counter\n",
    "                id_counter+=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "topicNum = 5\n",
    "for year, lda in LDA.items():\n",
    "    vectors[year] = []\n",
    "    for tn in tqdm(range(topicNum)):\n",
    "        elements = lda.show_topic(tn, topn=lda.num_terms)\n",
    "        vectors[year].append([(word2id[element[0]], element[1]) for element in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matutils.cossim(vectors[\"total\"][1], vectors[\"1900~1980\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectors[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in vectors:\n",
    "    for j in range(5):\n",
    "        columns.append(f'{i} (topic{j})')\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for j in range(25):\n",
    "    df.loc[j] = [0 for k in range(25)]\n",
    "df.index = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = vectors.copy()\n",
    "vec2 = vectors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = 5\n",
    "for ind1, year1 in enumerate(vec1): #row - first level\n",
    "    for ind2, year2 in enumerate(vec2): #column - first level\n",
    "            for ind11, tn1 in enumerate(range(topic_num)): #row - second level\n",
    "                for ind22, tn2 in enumerate(range(topic_num)): #column - second level\n",
    "                        sim = matutils.cossim(vec1[year1][tn1], vec2[year2][tn2])\n",
    "                        df.loc[f'{year1} (topic{tn1})'][f'{year2} (topic{tn2})'] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"LDA_MatrixForUncertainty.csv\", sep=\";\", decimal=\",\", float_format=\".3f\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a chart to display dynamic of change in novelty level with two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"LDA_MatrixForUncertainty.csv\", sep=\";\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=[\"Period\", \"Maximum Novelty\", \"Average Novelty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[0]=[\"P1\", 0.57, 0.68]\n",
    "df2.loc[1]=[\"P2\", 0.66, 0.70]\n",
    "df2.loc[2]=[\"P3\", 0.27, 0.48]\n",
    "df2.loc[3]=[\"P4\", 0.13, 0.41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot()\n",
    "plt.ylabel(\"Novelty level\")\n",
    "plt.xlabel(\"Periods\")\n",
    "plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.savefig(\"_sns_noveltyLevel.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=[\"period\", \"uncertainty\"])\n",
    "df3.loc[0]=[\"P0\", 0.57]\n",
    "df3.loc[1]=[\"P1\", 0.63]\n",
    "df3.loc[2]=[\"P2\", 0.41]\n",
    "df3.loc[3]=[\"P3\", 0.12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.plot(legend=False)\n",
    "plt.ylabel(\"uncertainty level\")\n",
    "plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
